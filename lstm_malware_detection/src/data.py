import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from keras.utils.data_utils import pad_sequences
from keras.preprocessing.text import Tokenizer

from sklearn.model_selection import train_test_split

class Preprocessing:

    def __init__(self, args) -> None:
        self.calls_data = "calls.zip"
        self.labels_data = "types.zip"
        self.max_len = args.max_len
        self.max_words = args.max_words
        self.test_size = args.test_size
        
    def load_data(self):
        malware_calls_df = pd.read_csv(self.calls_data, compression="zip", sep="\t", names=["API_Calls"])
        malware_labels_df = pd.read_csv(self.labels_data, compression="zip", sep="\t", names=["API_Labels"])
        
        malware_calls_df["API_Labels"] = malware_labels_df.API_Labels
        malware_calls_df["API_Calls"] = malware_calls_df["API_Calls"].apply(lambda x : " ".join(x.split(",")))
        malware_calls_df["API_Labels"] = malware_calls_df["API_Labels"].apply(lambda x: 1 if x == "Virus" else 0)
        
        X = malware_calls_df["API_Calls"].values
        Y = malware_calls_df["API_Labels"].values
        
        print(X)
        print(Y)
        
        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)
        print("Loaded %d into train_dataset, %d into test_dataset" % (len(self.x_train), len(self.x_test)))
        
    def prepare_tokens(self):
        self.tokens = Tokenizer(num_words=self.max_words)
        self.tokens.fit_on_texts(self.x_train)
        
    def sequence_to_token(self, x):
        sequences = self.tokens.texts_to_sequences(x)
        return pad_sequences(sequences, maxlen=self.max_len)