import torch
import torch.nn as nn
import torch.nn.functional as F

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class LSTMModel(nn.Module):
    def __init__(self, args) -> None:
        super(LSTMModel, self).__init__()
        self.embedding_dim = args.embedding_dim
        self.hidden_dim = args.hidden_dim
        self.batch_size = args.batch_size
        self.LSTM_layers = args.lstm_layers
        self.input_size = args.max_words
        
        self.dropout = nn.Dropout(0.5)
        self.embedding = nn.Embedding(self.input_size, self.embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            input_size=self.embedding_dim,
            hidden_size=self.hidden_dim,
            num_layers=self.LSTM_layers,
            bidirectional=True,
            batch_first=True,
            dropout=0.1
        )
        self.attention = nn.Linear(self.hidden_dim * 2, self.hidden_dim * 2)
        self.fc1 = nn.Linear(self.hidden_dim * 2, 257)
        self.fc2 = nn.Linear(257, 1)
        
    
    def forward(self, x):
        hidden_cell = (torch.zeros(self.LSTM_layers * 2, x.size(0), self.hidden_dim).to(device),
                       torch.zeros(self.LSTM_layers * 2, x.size(0), self.hidden_dim).to(device))
        
        embedded = self.embedding(x)
        output, _ = self.lstm(embedded, hidden_cell)
        attention_weights = torch.softmax(self.attention(output), dim=1)
        attention_output = torch.sum(attention_weights * output, dim=1)
        output = torch.relu_(self.fc1(attention_output))
        output = self.dropout(output)
        output = torch.sigmoid(self.fc2(output))
        
        return output

# class LSTMModel(nn.ModuleList):

# 	def __init__(self, args):
# 		super(LSTMModel, self).__init__()
		
# 		self.batch_size = args.batch_size
# 		self.hidden_dim = args.hidden_dim
# 		self.LSTM_layers = args.lstm_layers
# 		self.input_size = args.max_words # embedding dimention
		
# 		self.dropout = nn.Dropout(0.5)
# 		self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)
# 		self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)
# 		self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)
# 		self.fc2 = nn.Linear(257, 1)
		
# 	def forward(self, x):
	
# 		h = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).to(device)
# 		c = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim)).to(device)
		
# 		torch.nn.init.xavier_normal_(h)
# 		torch.nn.init.xavier_normal_(c)

# 		out = self.embedding(x)
# 		out, (hidden, cell) = self.lstm(out, (h,c))
# 		out = self.dropout(out)
# 		out = torch.relu_(self.fc1(out[:,-1,:]))
# 		out = self.dropout(out)
# 		out = torch.sigmoid(self.fc2(out))

# 		return out